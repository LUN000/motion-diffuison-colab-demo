{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dMP6nSlZR8_g"
      },
      "source": [
        "# Motion-diffusion Text2Motion Demo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v1XyyBvfR5Be"
      },
      "source": [
        "## Setup packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PeCGf9O-Rz4W"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import gdown\n",
        "!git clone https://github.com/GuyTevet/motion-diffusion-model.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SCByxWwgavX5"
      },
      "outputs": [],
      "source": [
        "os.chdir(\"motion-diffusion-model\")\n",
        "!sudo apt install ffmpeg\n",
        "\n",
        "# setup python pakage\n",
        "!pip install --upgrade --no-cache-dir gdown\n",
        "!gdown \"https://drive.google.com/uc?id=1p062yytbpR4U5Lpr6OE-5lf-J5IWWVjy\"\n",
        "!pip install -r requirements.txt\n",
        "\n",
        "# setup nlp lib\n",
        "!python3 -m spacy download en_core_web_sm\n",
        "!pip install git+https://github.com/openai/CLIP.git\n",
        "!bash prepare/download_glove.sh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F8EBYC5ZDapE"
      },
      "outputs": [],
      "source": [
        "# download smpl files\n",
        "!mkdir -p body_models\n",
        "os.chdir(\"body_models\")\n",
        "\n",
        "!echo -e \"The smpl files will be stored in the 'body_models/smpl/' folder\\n\"\n",
        "!wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1INYlGA76ak_cKGzvpOV2Pe6RkYTlXTW2' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1INYlGA76ak_cKGzvpOV2Pe6RkYTlXTW2\" -O smpl.zip && rm -rf /tmp/cookies.txt\n",
        "!rm -rf smpl\n",
        "\n",
        "!unzip smpl.zip\n",
        "!echo -e \"Cleaning\\n\"\n",
        "!rm smpl.zip\n",
        "\n",
        "!echo -e \"Downloading done!\"\n",
        "os.chdir(\"..\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r9PxdbqmDAjV"
      },
      "outputs": [],
      "source": [
        "# download t2m evaluators\n",
        "!echo -e \"Downloading T2M evaluators\"\n",
        "!wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1DSaKqWX2HlwBtVH5l7DdW96jeYUIXsOP' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1DSaKqWX2HlwBtVH5l7DdW96jeYUIXsOP\" -O t2m.zip && rm -rf /tmp/cookies.txt\n",
        "!rm -rf t2m\n",
        "\n",
        "!unzip t2m.zip\n",
        "!echo -e \"Cleaning\\n\"\n",
        "!rm t2m.zip\n",
        "\n",
        "!echo -e \"Downloading done!\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y7q92dzbU0tq"
      },
      "source": [
        "### Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-DgPzVbTSWBD"
      },
      "outputs": [],
      "source": [
        "os.chdir(\"..\")\n",
        "!git clone https://github.com/EricGuo5513/HumanML3D.git\n",
        "!unzip ./HumanML3D/HumanML3D/texts.zip -d ./HumanML3D/HumanML3D/\n",
        "!cp -r HumanML3D/HumanML3D motion-diffusion-model/dataset/HumanML3D"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uc_doVdZUfZH"
      },
      "source": [
        "### Pretrain model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8-2CluqBUC9J"
      },
      "outputs": [],
      "source": [
        "os.chdir(\"motion-diffusion-model\")\n",
        "!wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=FILEID' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1f8Sd_G53nWxDMvkTnzoouY-8xwOSiW0W\" -O model.zip && rm -rf /tmp/cookies.txt\n",
        "!unzip model.zip -d ./save/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2DSic0WuWTdl"
      },
      "source": [
        "## Generate\n",
        "generate with one line text\n",
        "- motion_length 動作秒數 2~9.8s\n",
        "- tetxt_prompt 句子\n",
        "- num repetitions 生成數量\n",
        "- gpu 是否使用 gpu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PgFOo0pwN2Bi"
      },
      "outputs": [],
      "source": [
        "seed = 12\n",
        "num_repetitions = 3\n",
        "motion_length = 6\n",
        "text = \" a man steps forward and does a handstand.\"\n",
        "gpu = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wHKo44YtV8DJ"
      },
      "outputs": [],
      "source": [
        "!mkdir \"../gen\"\n",
        "# generate location npy and mp4\n",
        "!python3 -m sample.generate --model_path ./save/humanml_trans_enc_512/model000200000.pt \\\n",
        "    --seed $seed\\\n",
        "    --cuda $gpu\\\n",
        "    --text_prompt \"${text}\" \\\n",
        "    --motion_length $motion_length \\\n",
        "    --num_repetitions $num_repetitions \\\n",
        "    --output_dir \"../gen\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tf2M1XyN-LsV"
      },
      "source": [
        "## Visualize bone video"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gh2d4NCVzHte"
      },
      "outputs": [],
      "source": [
        "from ipywidgets import Output, GridspecLayout\n",
        "from IPython import display\n",
        "\n",
        "os.chdir(\"..\")\n",
        "grid = GridspecLayout(num_repetitions+2, 1)\n",
        "\n",
        "i = 0\n",
        "for df in os.listdir(\"gen\"):\n",
        "    out = Output()\n",
        "    if df.endswith(\".mp4\"):\n",
        "      with out:\n",
        "          display.display(display.Video(os.path.join(\"gen\", df), embed=True))\n",
        "      grid[i, 0] = out\n",
        "      i += 1\n",
        "os.chdir(\"motion-diffusion-model\")\n",
        "grid"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Bones npy to gltf\n",
        "輸出 gltf 檔案\n",
        "- target 指定輸出第幾個sample"
      ],
      "metadata": {
        "id": "ZvZlKPsNW13A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pygltflib\n",
        "import base64\n",
        "import numpy as np\n",
        "import pygltflib\n",
        "from pygltflib import GLTF2, Scene"
      ],
      "metadata": {
        "id": "JgkoP3zvuu0F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Chidren connect ver"
      ],
      "metadata": {
        "id": "3mBH7ME979N1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nodes = [\n",
        " {\"children\": [1],\n",
        "  'name': 'Root'},\n",
        " {\"children\": [2,3,4],\n",
        "  'name': 'MidHip'},\n",
        " {\"children\": [5],\n",
        "  'name': 'LHip'},\n",
        " {\"children\": [6],\n",
        "  'name': 'RHip'},\n",
        " {\"children\": [7],\n",
        "  'name': 'spine1'},\n",
        " {\"children\": [8],\n",
        "  'name': 'LKnee'},\n",
        " {\"children\": [9],\n",
        "  'name': 'RKnee'},\n",
        " {\"children\": [10],\n",
        "  'name': 'spine2'},\n",
        " {\"children\": [11],\n",
        "  'name': 'LAnkle'},\n",
        " {\"children\": [12],\n",
        "  'name': 'RAnkle'},\n",
        " {\"children\": [13,14,15],\n",
        "  'name': 'spine3'},\n",
        " {'name': 'LFoot'},\n",
        " {'name': 'RFoot'},\n",
        " {\"children\": [16],\n",
        "  'name': 'Neck'},\n",
        " {\"children\": [17],\n",
        "  'name': 'LCollar'},\n",
        " {\"children\": [18],\n",
        "  'name': 'Rcollar'},\n",
        " {'name': 'Head'},\n",
        " {\"children\": [19],\n",
        "  'name': 'LShoulder'},\n",
        " {\"children\": [20],\n",
        "  'name': 'RShoulder'},\n",
        " {\"children\": [21],\n",
        "  'name': 'LElbow'},\n",
        " {\"children\": [22],\n",
        "  'name': 'RElbow'},\n",
        " {'name': 'LWrist'},\n",
        " {'name': 'RWrist'}\n",
        " ]\n",
        "\n",
        "scene_nodes = list(range(len(nodes)))"
      ],
      "metadata": {
        "id": "2I7MmZ8i786u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "npy = np.load(\"../gen/results.npy\", allow_pickle=True).tolist() # shape (repetitions, points, (x,y,z), frames)\n",
        "target = 0 \n",
        "motion = np.append([np.zeros((3, 120))], (npy['motion'][target]), axis=0) # Add root node and set to (0,0,0)\n",
        "frames = npy['lengths'][0]\n",
        "\n",
        "\n",
        "# append all nodes location data except root\n",
        "parent_dict = {}\n",
        "for i, node in enumerate(nodes):\n",
        "    for child in node.get(\"children\", []):\n",
        "        parent_dict[child] = i\n",
        "\n",
        "points = []\n",
        "for node in range(len(nodes)):\n",
        "  x, y, z = motion[node]\n",
        "  px, py, pz = motion[parent_dict[node]] if node != 0 else np.zeros((3,frames))\n",
        "  for i in range(frames):\n",
        "    points.append((x[i]-px[i], y[i]-py[i], z[i]-pz[i]))\n",
        "\n",
        "points = np.array(points, dtype=np.float32)\n",
        "times = np.array(np.arange(0, motion_length, motion_length/frames), dtype=np.float32)"
      ],
      "metadata": {
        "id": "FP0Lf0ng8GIf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def xyz2gltf(scene_nodes, nodes, points, times, save_path):\n",
        "  gltf = GLTF2()\n",
        "  gltf.scene = 0\n",
        "\n",
        "  # scene\n",
        "  scene = Scene(nodes=[0])\n",
        "  gltf.scenes.append(scene)\n",
        "  gltf.nodes = nodes\n",
        "\n",
        "  # buffer\n",
        "  uri = \"data:application/octet-stream;base64,\" + base64.b64encode(times).decode() + base64.b64encode(points).decode()\n",
        "  buf = pygltflib.Buffer(uri=uri, byteLength=times.nbytes+points.nbytes)\n",
        "  gltf.buffers.append(buf)\n",
        "\n",
        "  # bufferview\n",
        "  buf_view_time = pygltflib.BufferView(buffer=0, byteLength=times.nbytes, name=\"bufferViewAnimationFloatScalar\")\n",
        "  buf_view_point = pygltflib.BufferView(buffer=0, byteOffset=times.nbytes, byteLength=points.nbytes, name=\"bufferViewAnimationFloatVec3\")\n",
        "  gltf.bufferViews.append(buf_view_time)\n",
        "  gltf.bufferViews.append(buf_view_point)\n",
        "\n",
        "  # accessor\n",
        "  time_accessor = pygltflib.Accessor(\n",
        "        bufferView = 0,\n",
        "        componentType = 5126,\n",
        "        count = int(times.nbytes/4),\n",
        "        max = [\n",
        "          float(times[-1])\n",
        "        ],\n",
        "        min = [\n",
        "          0\n",
        "        ],\n",
        "        type = \"SCALAR\",\n",
        "        name = \"accessorAnimationInput\"\n",
        "      )\n",
        "  gltf.accessors.append(time_accessor)\n",
        "\n",
        "  byteOffset = 0\n",
        "  count=int(points.nbytes/(12*len(scene_nodes)))\n",
        "  for i in range(len(scene_nodes)):\n",
        "    accessor = pygltflib.Accessor(\n",
        "        bufferView=1,\n",
        "        byteOffset=byteOffset,\n",
        "        componentType=5126,\n",
        "        count=int(points.nbytes/(12*len(scene_nodes))),\n",
        "        type='VEC3',\n",
        "        name=\"accessorAnimationPositions\"\n",
        "    )\n",
        "    byteOffset += count*12\n",
        "    gltf.accessors.append(accessor)\n",
        "\n",
        "\n",
        "  #animaiton\n",
        "  channels, samplers = [], []\n",
        "  for i in range(len(scene_nodes)):\n",
        "    sampler = pygltflib.AnimationSampler(input=0, output=i+1)\n",
        "    channeltarget = pygltflib.AnimationChannelTarget(node=i, path=\"translation\")\n",
        "    channel = pygltflib.AnimationChannel(sampler=i, target=channeltarget)\n",
        "\n",
        "    channels.append(channel)\n",
        "    samplers.append(sampler)\n",
        "\n",
        "  animation = pygltflib.Animation(name=\"All Animations\", channels=channels, samplers=samplers)\n",
        "  gltf.animations.append(animation)\n",
        "\n",
        "  gltf.save(f\"{save_path}.gltf\")\n",
        "  print(f\"save at {save_path}\")\n",
        "  return gltf\n",
        "\n",
        "gltf = xyz2gltf(scene_nodes, nodes, points, times, \"../gen/result_child\")"
      ],
      "metadata": {
        "id": "uoF3mIVB9MI1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###No chidren ver"
      ],
      "metadata": {
        "id": "SfrulSNVC4_f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "AMASS_JOINT_MAP = {\n",
        "'MidHip': 0,\n",
        "'LHip': 1, 'LKnee': 4, 'LAnkle': 7, 'LFoot': 10,\n",
        "'RHip': 2, 'RKnee': 5, 'RAnkle': 8, 'RFoot': 11,\n",
        "'LShoulder': 16, 'LElbow': 18, 'LWrist': 20,  \n",
        "'RShoulder': 17, 'RElbow': 19, 'RWrist': 21, \n",
        "'spine1': 3, 'spine2': 6, 'spine3': 9,  'Neck': 12, 'Head': 15,\n",
        "'LCollar':13, 'Rcollar' :14, \n",
        "}\n",
        "scene_nodes = list(range(len(AMASS_JOINT_MAP)))\n",
        "inv_map = {v: k for k, v in AMASS_JOINT_MAP.items()}\n",
        "nodes = list(map(lambda x:{'name':inv_map[x]}, scene_nodes))"
      ],
      "metadata": {
        "id": "8s4UrPJs-5nY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "npy = np.load(\"../gen/results.npy\", allow_pickle=True).tolist() # shape (repetitions, points, (x,y,z), frames)\n",
        "keys = npy.keys()\n",
        "motion = npy['motion']\n",
        "frames = npy['lengths'][0]\n",
        "target = 2\n",
        "points = []\n",
        "\n",
        "for node in range(22):\n",
        "  x, y, z = motion[target][node]\n",
        "  for i in range(frames):\n",
        "    points.append((x[i], y[i], z[i]))\n",
        "\n",
        "points = np.array(points, dtype=np.float32)\n",
        "times = np.array(np.arange(0, motion_length, motion_length/frames), dtype=np.float32)"
      ],
      "metadata": {
        "id": "JUeTFi3P_BUQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def xyz2gltf(scene_nodes, nodes, points, times, save_path):\n",
        "  gltf = GLTF2()\n",
        "  gltf.scene = 0\n",
        "\n",
        "  # scene\n",
        "  scene = Scene(nodes=scene_nodes)\n",
        "  gltf.scenes.append(scene)\n",
        "  gltf.nodes = nodes\n",
        "\n",
        "  # buffer\n",
        "  uri = \"data:application/octet-stream;base64,\" + base64.b64encode(times).decode() + base64.b64encode(points).decode()\n",
        "  buf = pygltflib.Buffer(uri=uri, byteLength=times.nbytes+points.nbytes)\n",
        "  gltf.buffers.append(buf)\n",
        "\n",
        "  # bufferview\n",
        "  buf_view_time = pygltflib.BufferView(buffer=0, byteLength=times.nbytes, name=\"bufferViewAnimationFloatScalar\")\n",
        "  buf_view_point = pygltflib.BufferView(buffer=0, byteOffset=times.nbytes, byteLength=points.nbytes, name=\"bufferViewAnimationFloatVec3\")\n",
        "  gltf.bufferViews.append(buf_view_time)\n",
        "  gltf.bufferViews.append(buf_view_point)\n",
        "\n",
        "  # accessor\n",
        "  time_accessor = pygltflib.Accessor(\n",
        "        bufferView = 0,\n",
        "        componentType = 5126,\n",
        "        count = int(times.nbytes/4),\n",
        "        max = [\n",
        "          float(times[-1])\n",
        "        ],\n",
        "        min = [\n",
        "          0\n",
        "        ],\n",
        "        type = \"SCALAR\",\n",
        "        name = \"accessorAnimationInput\"\n",
        "      )\n",
        "  gltf.accessors.append(time_accessor)\n",
        "\n",
        "  byteOffset = 0\n",
        "  count=int(points.nbytes/(12*len(scene_nodes)))\n",
        "  for i in range(len(scene_nodes)):\n",
        "    accessor = pygltflib.Accessor(\n",
        "        bufferView=1,\n",
        "        byteOffset=byteOffset,\n",
        "        componentType=5126,\n",
        "        count=int(points.nbytes/(12*len(scene_nodes))),\n",
        "        type='VEC3',\n",
        "        name=\"accessorAnimationPositions\"\n",
        "    )\n",
        "    byteOffset += count*12\n",
        "    gltf.accessors.append(accessor)\n",
        "\n",
        "\n",
        "  #animaiton\n",
        "  channels, samplers = [], []\n",
        "  for i in range(len(scene_nodes)):\n",
        "    sampler = pygltflib.AnimationSampler(input=0, output=i+1)\n",
        "    channeltarget = pygltflib.AnimationChannelTarget(node=i, path=\"translation\")\n",
        "    channel = pygltflib.AnimationChannel(sampler=i, target=channeltarget)\n",
        "\n",
        "    channels.append(channel)\n",
        "    samplers.append(sampler)\n",
        "\n",
        "  animation = pygltflib.Animation(name=\"All Animations\", channels=channels, samplers=samplers)\n",
        "  gltf.animations.append(animation)\n",
        "\n",
        "  gltf.save(f\"{save_path}.gltf\")\n",
        "  print(f\"save at {save_path}\")\n",
        "  return gltf"
      ],
      "metadata": {
        "id": "XHcF2kxOML-B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gltf = xyz2gltf(scene_nodes, nodes, points, times, \"../gen/result_nochild\")"
      ],
      "metadata": {
        "id": "xu8YTbFkTEhn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## E2Pose npy to gltf"
      ],
      "metadata": {
        "id": "-fhaMBaVDriZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nodes = [\n",
        " {\"children\": [1],\n",
        "  'name':'Root'},\n",
        " {\"children\": [2,5,8],\n",
        "  'name': 'Hip'},\n",
        " {\"children\": [3],\n",
        "  'name': 'rhip'},\n",
        " {\"children\": [4],\n",
        "  'name': 'rknee'},\n",
        " {'name': 'rfoot'},\n",
        " {\"children\": [6],\n",
        "  'name': 'lhip'},\n",
        " {\"children\": [7],\n",
        "  'name': 'lknee'},\n",
        " {'name': 'lfoot'},\n",
        " {\"children\": [9],\n",
        "  'name': 'spine'},\n",
        " {\"children\": [10,12,15],\n",
        "  'name': 'spine2'},\n",
        " {\"children\": [11],\n",
        "  'name': 'knee'},\n",
        " {'name': 'head'},\n",
        " {\"children\": [13],\n",
        "  'name': 'lshoulder'},\n",
        " {\"children\": [14],\n",
        "  'name': 'lelbow'},\n",
        " {'name': 'lhand'},\n",
        " {\"children\": [16],\n",
        "  'name': 'rshoulder'},\n",
        " {\"children\": [17],\n",
        "  'name': 'relbow'},\n",
        " {'name': 'rhand'}]\n",
        "\n",
        "scene_nodes = list(range(len(nodes)))"
      ],
      "metadata": {
        "id": "tzZtGy4CRnpn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "video23d = np.load(\"../VideoTo3D_dance.npy\", allow_pickle=True)\n",
        "video23d = np.append(np.zeros((len(video23d), 1, 3)), video23d, axis=1)\n",
        "\n",
        "parent_dict = {}\n",
        "for i, node in enumerate(nodes):\n",
        "    for child in node.get(\"children\", []):\n",
        "        parent_dict[child] = i\n",
        "\n",
        "points = []\n",
        "for node in range(len(nodes)):\n",
        "  for i in range(300, 900):\n",
        "    x ,y, z = video23d[i][node]\n",
        "    px, py, pz = video23d[i][parent_dict[node]] if node != 0 else np.zeros(3)\n",
        "    points.append((x-px, -y+py, z-pz))\n",
        "\n",
        "points = np.array(points, dtype=np.float32)\n",
        "times = np.array(np.arange(0, 20, 1/30), dtype=np.float32)"
      ],
      "metadata": {
        "id": "TLZh6-oNTdm9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def xyz2gltf(scene_nodes, nodes, points, times, save_path):\n",
        "  gltf = GLTF2()\n",
        "  gltf.scene = 0\n",
        "\n",
        "  # scene\n",
        "  scene = Scene(nodes=[0])\n",
        "  gltf.scenes.append(scene)\n",
        "  gltf.nodes = nodes\n",
        "\n",
        "  # buffer\n",
        "  uri = \"data:application/octet-stream;base64,\" + base64.b64encode(times).decode() + base64.b64encode(points).decode()\n",
        "  buf = pygltflib.Buffer(uri=uri, byteLength=times.nbytes+points.nbytes)\n",
        "  gltf.buffers.append(buf)\n",
        "\n",
        "  # bufferview\n",
        "  buf_view_time = pygltflib.BufferView(buffer=0, byteLength=times.nbytes, name=\"bufferViewAnimationFloatScalar\")\n",
        "  buf_view_point = pygltflib.BufferView(buffer=0, byteOffset=times.nbytes, byteLength=points.nbytes, name=\"bufferViewAnimationFloatVec3\")\n",
        "  gltf.bufferViews.append(buf_view_time)\n",
        "  gltf.bufferViews.append(buf_view_point)\n",
        "\n",
        "  # accessor\n",
        "  time_accessor = pygltflib.Accessor(\n",
        "        bufferView = 0,\n",
        "        componentType = 5126,\n",
        "        count = int(times.nbytes/4),\n",
        "        max = [\n",
        "          float(times[-1])\n",
        "        ],\n",
        "        min = [\n",
        "          0\n",
        "        ],\n",
        "        type = \"SCALAR\",\n",
        "        name = \"accessorAnimationInput\"\n",
        "      )\n",
        "  gltf.accessors.append(time_accessor)\n",
        "\n",
        "  byteOffset = 0\n",
        "  count=int(points.nbytes/(12*len(scene_nodes)))\n",
        "  for i in range(len(scene_nodes)):\n",
        "    accessor = pygltflib.Accessor(\n",
        "        bufferView=1,\n",
        "        byteOffset=byteOffset,\n",
        "        componentType=5126,\n",
        "        count=int(points.nbytes/(12*len(scene_nodes))),\n",
        "        type='VEC3',\n",
        "        name=\"accessorAnimationPositions\"\n",
        "    )\n",
        "    byteOffset += count*12\n",
        "    gltf.accessors.append(accessor)\n",
        "\n",
        "\n",
        "  #animaiton\n",
        "  channels, samplers = [], []\n",
        "  for i in range(len(scene_nodes)):\n",
        "    sampler = pygltflib.AnimationSampler(input=0, output=i+1)\n",
        "    channeltarget = pygltflib.AnimationChannelTarget(node=i, path=\"translation\")\n",
        "    channel = pygltflib.AnimationChannel(sampler=i, target=channeltarget)\n",
        "\n",
        "    channels.append(channel)\n",
        "    samplers.append(sampler)\n",
        "\n",
        "  animation = pygltflib.Animation(name=\"All Animations\", channels=channels, samplers=samplers)\n",
        "  gltf.animations.append(animation)\n",
        "\n",
        "  gltf.save(f\"{save_path}.gltf\")\n",
        "  print(f\"save at {save_path}\")\n",
        "  return gltf"
      ],
      "metadata": {
        "id": "VXbCH_NKYK_C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gltf = xyz2gltf(scene_nodes, nodes, points, times, \"../gen/E2pose\")"
      ],
      "metadata": {
        "id": "9B7Wun96IiSd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generate mesh glb file (unnecessary)"
      ],
      "metadata": {
        "id": "bUmAVrmJfR0t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from model.rotation2xyz import Rotation2xyz\n",
        "import numpy as np\n",
        "from trimesh import Trimesh\n",
        "import os\n",
        "import torch\n",
        "from visualize.simplify_loc2rot import joints2smpl\n",
        "\n",
        "class npy2obj:\n",
        "    def __init__(self, npy_path, sample_idx, rep_idx, device=0, cuda=True):\n",
        "        self.npy_path = npy_path\n",
        "        self.motions = np.load(self.npy_path, allow_pickle=True)\n",
        "        if self.npy_path.endswith('.npz'):\n",
        "            self.motions = self.motions['arr_0']\n",
        "        self.motions = self.motions[None][0]\n",
        "        self.rot2xyz = Rotation2xyz(device='cpu')\n",
        "        self.faces = self.rot2xyz.smpl_model.faces\n",
        "        self.bs, self.njoints, self.nfeats, self.nframes = self.motions['motion'].shape\n",
        "        self.opt_cache = {}\n",
        "        self.sample_idx = sample_idx\n",
        "        self.total_num_samples = self.motions['num_samples']\n",
        "        self.rep_idx = rep_idx\n",
        "        self.absl_idx = self.rep_idx*self.total_num_samples + self.sample_idx\n",
        "        self.num_frames = self.motions['motion'][self.absl_idx].shape[-1]\n",
        "        self.j2s = joints2smpl(num_frames=self.num_frames, device_id=device, cuda=cuda)\n",
        "\n",
        "        if self.nfeats == 3:\n",
        "            print(f'Running SMPLify For sample [{sample_idx}], repetition [{rep_idx}], it may take a few minutes.')\n",
        "            motion_tensor, opt_dict = self.j2s.joint2smpl(self.motions['motion'][self.absl_idx].transpose(2, 0, 1))  # [nframes, njoints, 3]\n",
        "            self.motions['motion'] = motion_tensor.cpu().numpy()\n",
        "        elif self.nfeats == 6:\n",
        "            self.motions['motion'] = self.motions['motion'][[self.absl_idx]]\n",
        "        self.bs, self.njoints, self.nfeats, self.nframes = self.motions['motion'].shape\n",
        "        self.real_num_frames = self.motions['lengths'][self.absl_idx]\n",
        "\n",
        "        self.vertices = self.rot2xyz(torch.tensor(self.motions['motion']), mask=None,\n",
        "                                     pose_rep='rot6d', translation=True, glob=True,\n",
        "                                     jointstype='vertices',\n",
        "                                     # jointstype='smpl',  # for joint locations\n",
        "                                     vertstrans=True)\n",
        "        self.root_loc = self.motions['motion'][:, -1, :3, :].reshape(1, 1, 3, -1)\n",
        "        self.vertices += self.root_loc\n",
        "\n",
        "    def get_vertices(self, sample_i, frame_i):\n",
        "        return self.vertices[sample_i, :, :, frame_i].squeeze().tolist()\n",
        "\n",
        "    def get_trimesh(self, sample_i, frame_i):\n",
        "        return Trimesh(vertices=self.get_vertices(sample_i, frame_i),\n",
        "                       faces=self.faces)\n",
        "\n",
        "    def save_obj(self, save_path, frame_i):\n",
        "        mesh = self.get_trimesh(0, frame_i)\n",
        "        mesh.export(f'{save_path}.glb')\n",
        "        return save_path\n",
        "    \n",
        "    def save_npy(self, save_path):\n",
        "        data_dict = {\n",
        "            'motion': self.motions['motion'][0, :, :, :self.real_num_frames],\n",
        "            'thetas': self.motions['motion'][0, :-1, :, :self.real_num_frames],\n",
        "            'root_translation': self.motions['motion'][0, -1, :3, :self.real_num_frames],\n",
        "            'faces': self.faces,\n",
        "            'vertices': self.vertices[0, :, :, :self.real_num_frames],\n",
        "            'text': self.motions['text'][0],\n",
        "            'length': self.real_num_frames,\n",
        "        }\n",
        "        np.save(save_path, data_dict)\n",
        "        return"
      ],
      "metadata": {
        "id": "xIfAx4GEfrQc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import os\n",
        "from visualize import vis_utils\n",
        "import shutil\n",
        "from tqdm import tqdm\n",
        "\n",
        "def render_mesh(input_path):\n",
        "  parser = argparse.ArgumentParser()\n",
        "  parser.add_argument(\"--input_path\", type=str, required=True, help='stick figure mp4 file to be rendered.')\n",
        "  parser.add_argument(\"--cuda\", type=bool, default=True, help='')\n",
        "  parser.add_argument(\"--device\", type=int, default=0, help='')\n",
        "  params = parser.parse_args(args=[\"--input_path\", input_path])\n",
        "\n",
        "  assert params.input_path.endswith('.mp4')\n",
        "  parsed_name = os.path.basename(params.input_path).replace('.mp4', '').replace('sample', '').replace('rep', '')\n",
        "  sample_i, rep_i = [int(e) for e in parsed_name.split('_')]\n",
        "  npy_path = os.path.join(os.path.dirname(params.input_path), 'results.npy')\n",
        "  out_npy_path = params.input_path.replace('.mp4', '_smpl_params.npy')\n",
        "  assert os.path.exists(npy_path)\n",
        "  results_dir = params.input_path.replace('.mp4', '_glb')\n",
        "  if os.path.exists(results_dir):\n",
        "      shutil.rmtree(results_dir)\n",
        "  os.makedirs(results_dir)\n",
        "\n",
        "  npy2obj = vis_utils.npy2obj(npy_path, sample_i, rep_i,\n",
        "                              device=params.device, cuda=params.cuda)\n",
        "\n",
        "  print('Saving obj files to [{}]'.format(os.path.abspath(results_dir)))\n",
        "  for frame_i in tqdm(range(npy2obj.real_num_frames)):\n",
        "      npy2obj.save_obj(os.path.join(results_dir, 'frame{:03d}'.format(frame_i)), frame_i)\n",
        "\n",
        "  print('Saving SMPL params to [{}]'.format(os.path.abspath(out_npy_path)))\n",
        "  npy2obj.save_npy(out_npy_path)\n",
        "  return"
      ],
      "metadata": {
        "id": "9BRK7mooeFr8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "render_mesh(\"../gen/sample00_rep00.mp4\")"
      ],
      "metadata": {
        "id": "pPT30LIcftD5"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "v1XyyBvfR5Be",
        "2DSic0WuWTdl",
        "tf2M1XyN-LsV",
        "ZvZlKPsNW13A",
        "3mBH7ME979N1",
        "SfrulSNVC4_f",
        "-fhaMBaVDriZ",
        "bUmAVrmJfR0t"
      ],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}